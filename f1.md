## F1 

《F1: A Distributed SQL Database That Scales》

- Based on Spanner
- F1 servers are almost stateless
- The Spanner servers in each datacenter in turn retrieve their data from the Colossus File System (CFS) in the same datacenter. Unlike Spanner, CFS is not a globally replicated service and therefore Spanner servers will never communicate with remote CFS instances.
- F1 and Spanner were developed at the same time and in close collaboration.
- Spanner handles lower-level storage issues like persistence, caching, replication, fault tolerance, data sharding and movement, location lookups, and transactions.
- Spanner has very strong consistency and timestamp semantics. Every transaction is assigned a commit timestamp, and these timestamps provide a global total ordering for commits.
- In fact, Spanner’s original data model was more like Bigtable, but Spanner later adopted F1’s data model.
- F1 has a relational schema similar to that of a traditional RDBMS, with some extensions including explicit table hierarchy and columns with Protocol Buffer data types.
- There are two types of physical storage layout for F1 indexes: local and global.
- all schema changes fully non-blocking.
- Change History is a first-class feature at the database level, has a publish-and-subscribe system to push notifications that particular root rows have changed.
- For F1, common ORM anti-patterns are disastrous, the client must change.
- Because of F1’s network based storage, scheduling multiple data accesses in parallel often results in near-linear speedup until the underlying storage system is truly overloaded. 
- Intuitively, 3-way replication should suffice for high availability. In practice, this is not enough, F1 use 5-way.



